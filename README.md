# Function-Approximation
Function Approximation (Tic-tac-toe Game) 

Studying Chapter one of Tom Michaelâ€™s book is highly recommended for understanding the implementation of this mini project. In the first chapter of Tom Mitchelâ€™s book, the learning problem is reduced to a function approximation problem and one specific class of learning Task is chosen to explain in detail. This class is learning from training data with indirect feedback. This class of learning tasks is different from the class of Concept Learning. Concept learning is about learning from direct feedback.

In this project, I have used Mitchelâ€™s described approach to implement an algorithm to learn the tic-tac-toe game. As you know, such tasks abstract as a graph that every board state is a node and every move is an edge. The goal is to find the best move for each state. (You can find details of abstract representation of tic-tac-toe in Russellâ€™s book: artificial intelligent, a modern approach adversarial search Chapter). 
We can define a function ğ‘‰: ğ‘† â†’ â„ that assigns to every state a value and based on ğ‘‰ find the best move for each state. If we are able to find ğ‘‰, the problem is solved but only a few pairs < ğ‘ , ğ‘‰(ğ‘ ) > are available. There are many states that donâ€™t have the value of ğ‘‰. Therefore, we should approximate this function only by these points (training examples). Because of the graph structure of the problem, this is possible to approximate the value of unknown-value states and one can use approximated values to find better approximations. So for unknown-value states, you should use approximated successor stateâ€™s value.

# A) Implementing an algorithm, which learns to play Tic-tac-toe (3Ã— 3 board) with a function approximation approach.

Steps:

1. Choosing linear representation for ğ‘‰Ì‚ : ğ‘‰Ì‚(ğ‘¥âƒ—) = ğ‘¤âƒ—ğ‘‡ğ‘¥âƒ—.

2. Finding some good features (ğ‘¥âƒ—) to describe the states of tic-tac-toe.

3. Initializing all coefficients (ğ‘¤âƒ—) in ğ‘‰Ì‚ by zero.

4. Starting with an empty board state and in each non-terminal state (ğ‘ ğ‘¡) the move has been chosen based on ğ‘‰Ì‚ as follows: Among the moves, someone can do in the current state, choose the one which leads to the highest state value (value of successors of the current state). These chosen successor states are called  ğ‘ ğ‘¡+1 and assigned ğ‘‰ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘ ğ‘¡) = ğ‘‰Ì‚(ğ‘ ğ‘¡+1). If ğ‘ t is a terminal state, then we assign ğ‘‰ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘ ğ‘¡) = ğ‘‰(ğ‘ ) and set the value of the final state as follows:

    ğ‘‰ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘ ğ‘¡) = 100 If a player wins
    
    ğ‘‰ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘ ğ‘¡) = âˆ’100 If a player loses
    
    ğ‘‰ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘ ğ‘¡) = 0 If a player draws
    
5. Finally, we have new example: < ğ‘ ğ‘¡, ğ‘‰ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘ ğ‘¡) >. Based on this example, we update ğ‘¤âƒ— as follows:

    ğ‘¤ğ‘– = ğ‘¤ğ‘– + ğ›¼ (ğ‘‰ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘ ğ‘¡) âˆ’ ğ‘‰Ì‚(ğ‘ ğ‘¡)) ğ‘¥ğ‘–
    
I have assigned a small value for ğ›¼ to train the agent and run two agents to play with each other.

# B) Based on the previous part, an agent code has been written that you can play with it.

